% !TeX root = ..\essay.tex

\subsection{Step 3 - Creating Summaries from Hierarchies}
After the second step of the pipeline our system will have constructed hierarchies, as explained in the previous chapter. Each hierarchy file contains the relevant sentences of one of the topics. The hierarchies themselves consist of XML trees which should represents subtopics of the topics. The bubbles, which are nodes of the trees, can have multiple nuggets, which in our system are always complete sentences.

The task of the third pipeline step was now to construct summaries out of these hierarchy files. We focused on creating general summaries over the complete topic instead of only focusing on subtopics. 

The challenge is here the length of the summaries, because they should contain only about 600 characters. It is therefore important to select representative sentences and arrange them in a meaningful way, so that the end result has a high quality. 

Because our system uses complete sentences as nuggets, grammatically and spelling
where not a big problem, at least when the source texts were written well.

The first thing we concentrated on was to select sentences in a way that the structure of the summary feels natural and focused. To achieve this we arrived at the concept of traversing a hierarchy tree and it's nuggets in the same way a depth
first search would do it.

The idea behind this approach is that it resembles the way a human would talk about topics and subtopics. It starts with a really general
explanation of a topic and becomes more and more specific, before than switching to other subtopics, becoming there more and more specific and doing the same for other subtopics. This structure feels natural, since a human would not randomly jump between subtopics in a summary, but elaborate them one after another. When relevant sentences were selected in the first step of the pipeline and the end result of the second pipeline step has a high quality and therefore a good structure of the sentences, our summaries would automatically
also be really good.

During the development we first tested the third step with the given 10 gold standard hierarchies of the corpus, since the other steps were not completely implemented yet. Here the result was of course especially good, because the hierarchies of the corpus were done by three human annotators instead of computer-generated. When we later used the automatically generated hierarchies of our previous pipeline steps, the result thankfully still seemed to be acceptably good, which proofed the quality of the other pipeline steps.

The challenge was now of course to create summaries that had only a specific length. We cut the deeper bubbles of the hierarchy until the nuggets would sum up to the requested summary length. This means that the summaries would still contain the general information about the subtopics but not became
as specific about each.

After it was revealed that the amount of characters per summary should not be
higher than 600, which depending on the sentence length most of the time
corresponded to about 4 to 8 sentences, our approach had to be changed because such very short summaries would result in the depth first search only looking into one subtopic, and even
there just in the general nuggets, without reaching the other subtopics.

Our new approach was then to order the trees according to their size and selecting from the largest trees nuggets of the roots. The idea behind this is that a large tree size (measured by the amount of nuggets in the tree) means that the subtopic represented by this tree contains a lot of information and should therefore be included in the summary. From each tree root we selected the shortest sentence and included it into the summary. By doing this we could cover more subtrees and had therefore a wider range of information that could be covered. Selecting the sentences from the roots was done because for such a short summary it would not make sense to go into detail and take sentences from deeper within the hierarchies. 

This new approach still resulted in good summaries. They where unfortunately not as much focused and
good structured anymore as before, because they jumped from subtopic to subtopic but for a short summaries about the complete topic this is of course unavoidable.

Now that we had created our first relatively simple baseline summary we tried using some of the recommended tools to figure out if we could gain any advantage from them regarding the n the end rated criteria that will be further explained in section 3.1. These tools will automatically create a summary out of a given text each of them based on a specific algorithm. 

To obtain the input text we went a step back and created a continous text out of all the nuggets contained in the hirarchy by concatinating them according to the way a depth first search would traverse the individual bubbles. In doing so we could give the tool the complete information that was classified as relevant and extracted in step 1 and provided the possibility that the tool could take important sentences from deep down in the hirarchy that would not have made it into the summary by using the approach that created our fist baseline. It should be mentioned that the input text was exactly the same for all the different tools so that each of them would start summarizing under equal preconditions and we could better compare the created summaries against each other. 

Unfortunately, would all of the tools limit the length of the result summaries either by taking a specific word count the final summary should not exceed or by shortening the input text to a specific ratio but not one of them was able to restrict the length based on a character count. 

To tackle this problem we set the word count for each tool to an amount so that the output summary would contain approximately 600 characters. Afterwards we would cut those sentences leading to more than the demanded amount of characters. Since doing so would sometimes lead to summaries consisting of only two sentences or having a character count of only 400 or less we started adding additional sentences from our hirarchy in a similiar way we did when we created our first baseline summary. This means that we would take the shortest sentence from every root bubble, order the subtrees according to their size and select the shortest sentence per bubble from the largest subtrees untill the maximum of 600 characters was reached.

This approach had the upside that the information content of our resulting summaries was significantly improved. However, since the last sentence of the summary that was produced by one of the tools was usually more specific on a subtopic and came from deeper down in the hirarchy going back to the more general sentences of the top level bubbles would also sometimes lead to a downgrade in coherence and structure which leaves room for future improvement.  

The first tool that was examined was the the summarizer of the gensim library created by \citet{rehurek_lrec} which implements its own version of the popular graph-based TextRank algorithm to extract the most important sentences in a text. In its original form this algorithm creates a graphic representation of the input text where the sentences are its nodes and uses a measure of how much content two sentences share as a similiarity function to create edges between them. The tool however instead of implementing the original TextRank makes use of one of the proposed variations of the similiarity function (BM25) that is utilized in constructing the graphic representation of the input text from the work of \citet{DBLP:journals/corr/BarriosLAW16}. 

Unfortunately, this did somehow not work well with our input at all resulting in summaries that even though they were grammatically and spelling wise correct were neither well structured and coherent nor especially focused and jumped from subtopic to subtopic in an uncoordinated way.  

Since we could not benefit from the gensim summarizer in any way we moved on and tried making use of the sumy library created by \citet{sumy}. This library contains various implementations of different algorithms that can be helpfull for the task of summarization. Due to the fact that there were so many different implemented approaches to summarization that we tested and going into detail for each one would be to long for this report we will list all of them with a short explanation of how they generally extract the important sentences from the input text respectively: 
\begin{itemize}
	\item \textbf{Luhn:} extracts salient sentences of an input text using features like high frequency words or phrases (thereby ignoring stopwords) and how often these appear in a sentence
	\item \textbf{Edmundson:} uses key phrases in addition to frequency based weights with three methods of determining the sentence weight: first, based on presence or absence of cue words in a cue dictionary. Second, sum of all content words content words appearing the title and headings of the text. Third, location of the sentence (i.e. sentences in the beginning of a text or paragraph are more important)
	\item \textbf{Kullback Leibler divergence:} a measure of difference between the true probability distribution of words of the input text and the approximated probability distribution of the summary; sentences are greedily added as long as they decrease the KL divergence since a lower value indicates that the importance of words in input text and summary is more similar
	\item \textbf{Latent Semantic Analysis:} first a matrix with unique words in rows, sentences of the input text in columns and cells containing the number of occurences of the word in the sentence is created; afterwards this matrix is transformed into  three matrices (word x concept, diagonal matrix with scaling values, sentences x concepts) through Singular Value Decomposition which is able to extract concepts of words; finally, you can take the sentence with the highest similiarity score per concept (e.g. computed with cosine similarity) from the transposed sentence x concepts matrix into the result summary 
	\item \textbf{TextRank:} builds a graphic representation of the input text as seen above and extracts themost important sentences by running the PageRank algorithm	
\end{itemize} 
Especially the approach using the Kullback Leibler divergence and the one using Latent Semantic Analysis produced acceptable output. However, both had the problem that the created summaries usually contained a good amount of information but would also often contain sentences with pronouns or names of unfamiliar people and thus lack in terms of refenrential clarity, focus and coherence. 

Because we were not fully satisfied with the results of any of the available implementations of the sumy library we continued searching for a tool that would work well with our input and eventually discovered the summa library with its own summarizer. Once again a version of the TextRank algorithm was implemented by this tool taking BM25 as the similiarity function for creating a graphic representation of the input text according to the work of \citet{DBLP:journals/corr/BarriosLAW16}. 

Despite the poor performance of the gensim summarizer which implemented a very similar version of the TextRank algorithm this most of the time produced very good summaries that were not only well focused, structured and coherent on the one hand but had a good information content on the other hand. Why this worked so well while the gensim summarizer did not we still have to figure out. Nevertheless, became those summaries our final state due to their good overall quality.  
 
Finally, we made the observation that there were multiple sentences of the form "X \textbf{is a} short explanation of X" (e.g. "ADHD is a brain-based disorder where the chemistry of the brain (neurotransmitters) is not functioning as it should.") for several of the topics. We extracted those so called "definition sentences" through a simple matching of the strings "is a " and "is an " for every topic and added them (if there existed such a sentence in the documents) as the first sentence of the corresponding summary. This did very much improve the structure on the one hand as well as the readabilty of our summaries on the other hand since it was now clarified what specific topic a summary was all about and shortly explained said topic in its beginning.
