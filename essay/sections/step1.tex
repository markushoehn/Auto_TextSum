% !TeX root = ..\essay.tex

\subsection{Step 1 - Content Selection}
In this step we dealt with choosing content from a set of given sentences and headings which is generally appropriate for a summarization. What we did can be divided in three parts. First we analyzed the data in order to gain some insights and orientate ourselves so that we know in what direction we can potentially go. After we found the general direction, we tried out four different approaches to retrieve nuggets and picked one of them. After it was fine tuned, we generated our nuggets as input to the next pipeline step.
\subsubsection{Data Analysis}
\textbf{Structure of nuggets}\\
First, we looked at how many nuggets are whole sentences. "Whole sentence", in this context, means that there is a 1:1 counterpart in the relevant sentences input file. Finding such a counterpart was actually nto trivial, since...\lbrack Manche Sätze waren doch quasi preprocessed "arent" -> "are nt", ist das Problem hier aufgetreten oder wo anders? Außerdem finde ich gerade kein Beispiel dafür, das würde ich hier aber angeben\rbrack. We solved this by using difflib \lbrack Link to website\rbrack, which gives a score of how similar two sequences (of characters) are. We chose a threshold of ???\% and declared every pair of sentences with a similarity above that threshold to be the same. As a result, we found that depending on the topic between 70.5\% to 87.9\% of nuggets are whole sentences with a mean of 82\%. This ratio was surprisingly high and led us to a big simplification we have made, which is to assume that all nuggets are always complete sentences. Therefore, the core problem we have to tackle is to decide whether a given sentence is a nugget or not.\\
We further analyzed the structure of nuggets by searching for patterns like certain words that appear primarily in nuggets or no nuggets. We thought that words like "significant" or "important" could indicate that there is relevant information content in this sentence and thus would make good features. Unfortunately, it turned out that words are very much equally distributed over both types of sentences. Which words appear most often is, as you would expect, topic dependend.
\textbf{Ratios of Nuggets}\\
Next, we analyzed how the proportions between nuggets and no nuggets are in the relevant sentences we were given as input. We found out that, depending on the topic, between 9.3\% to 24.9\% of sentences are nuggets with a mean of 14.1\%. Knowing the distribution of positive and negative examples, i.e. nuggets and no nuggets, is important, because it directly affects the learning process of the later classifier. If for example there are only 10\% positive examples,  a classifier may choose to always predict the majority class, which would be the negative one, no nugget. This leads to a high precision but is not what we want. When we know these numbers, we can multiply the amount of positive examples to reach something close to a 50:50 distribution. Hereby we would increase the cost of classifying a positive instance wrong and therefore have more control over the learning process of the classifier.

\subsection{training classifiers}
After gaining sufficient insight into the data we were given, we created binary labeled versions of them. For preprocessing we used gensims simple preprocess \lbrack cite/link!\rbrack .We thought about different approaches to train a classifier and ended up with four general variants.\\
\textbf{FastText}\\
FastText \cite{joulin2016bag} is an open-source library for text classification and word embeddings developed by Facebook AI Research (FAIR) lab. Since it already provides an implemenation for text classification using word embeddings, we just had to transform our data into the right format and potentially change the ratio of positive and negative examples in order to control the learning process of the classifier. \lbrack nachdem ich hier nochmal rumprobiert habe, bekomme ich Recall = 0.1617, Precision = 0.8175, was besser erscheint als es sein sollte! Was fangen wir jetzt damit an? \rbrack. Because of ... we did not trust these results?... \\
\textbf{Naive Bayes}\\
Naive Bayes was our only approach that did not involve word embeddings. As features we chose the words themselves, \lbrack did we?\rbrack , we also used the sentence length with discretized intervals and we used a POS tagger to create a binary feature of whether a verb is present or not. Unfortunately the results were very bad. That is because first of all, we did not find enough good features and second, the POS tagger could not reliably tell whether a sentence contained a verb or not.\\
This approach became interesting when we included 10 further features of which each one corresponded to one of the top 10 most frequent words in that topic and whether it is present in the sentence or not. This resulted in a recall of 37.8\% and a precision of 19.5\%, which is actually something you would be able to work with. The downside to this is, that you can train a classifier only for one specific topic and you need training data for it. So there is no way to retrieve nuggets for a topic without training data. In conclusion we can say, that this approach did not work at all.\\
\textbf{Fully Connected Neural Network}\\
In our neural network approaches we chose to represent words with 300 dimensional GloVe embeddings. In order to create a good representation for complete sentences instead of only words, we averaged the sum of all word embeddings of a sentence. We chose this representation because it does not require heavy computations and seems to still deliver good results as indicated by \cite{iyyer2015deep}. Furthermore, we decided to take into account the context of the sentence to be classified. In order to do this, the k preceding and succeeding sentences (according to the document in the input xml) are used as input as well.\\
We implemented our network with Keras and chose as parameters... . As a result we obtained a precision of 35\% and a recall of 1.5\%. In order to summarize a topic, these results are not sufficient at all.\\
\textbf{Convolutional Neural Network}\\
The architecture we chose in the end was a convolutional neural network. For this approach, we classified sentences in isolation instead of including the context and we did not represent the sentence as the average of its word embeddings but instead as a sequence of its word embeddings. We made more restrictions by excluding sentences with less than 5 and more than 50 words and padded each sentence to 50 words. We made this decision because we observed that phrases like \textit{U.S. History II}, \textit{I don't know.} or \textit{All-School Assemblies} cannot make good summaries and neither do sentences that span more than 50 words. We padded the input to 50 words because our architecture expects a fixed amount of them. We also filtered out sentences with containing phrases or characters like "I", "?", "We" in order to filter out personal opinions etc... .\\
In order to find the right parameters we conducted random search. After testing some diverse parameter configurations and got an idea of what generally works, we defined ranges for each of them. The program received these ranges, as well as the training, deveopment and validation set. It generated random configurations within the given ranges, trained the network on the training set and evaluated the model after each epoch. If performance decreased on the development set, the program finished training in order to prevent overfitting and to save ressources. If the new model beat the currently best one, it is stored and the best results are udated. Since this task is very computation heavy, we used the Lichtenberg Cluster \lbrack Link...\rbrack to do this for us. As a general restraint for the results, we decided to only allow models that have a recall of at least 5\% but a precision as high as possible. Recall and precision seem to behave anti-proportional, therefore we have to choose which one them is more important to us. Since we want to create meaningful and well readable summarizations in the end which are not very long anyway, precision is of a much greater importance to us. \\
\lbrack insert image of ranges and results of random search\rbrack \\
Our final parameters provided a recall of 5.9\% and a precision of 52\%. We noticed that for some topics \lbrack which?\rbrack the amount of predicted nuggets is very large and for some very small. This led us to the decision that we always use the 30 sentences with the highest probability of being nuggets as input to the next pipeline step. This guarantees enough material to work with in the 
later steps while using only the nuggets we are most confident in if we would have predicted more.\\
We tried improving the above results by weighting word embeddings with tf-idf weights, but they seem to only confuse the neural network. Results very terrible \lbrack what exactly did come out of this?\rbrack .\\
QUESTION: Split approach and evaluation into two parts? or discuss results directly after explaining the approach?

@article{joulin2016bag,
  title={Bag of Tricks for Efficient Text Classification},
  author={Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Mikolov, Tomas},
  journal={arXiv preprint arXiv:1607.01759},
  year={2016}
}

@inproceedings{iyyer2015deep,
  title={Deep unordered composition rivals syntactic methods for text classification},
  author={Iyyer, Mohit and Manjunatha, Varun and Boyd-Graber, Jordan and Daum{\'e} III, Hal},
  booktitle={Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  volume={1},
  pages={1681--1691},
  year={2015}
}
