% !TeX root = ..\essay.tex

\subsection{Step 1 - Content Selection}
In this step we dealt with choosing content from a set of given sentences and headings which is generally appropriate for a summarization. What we did can be divided in three parts. First we analyzed the data in order to gain some insights and orientate ourselves so that we know in what direction we can potentially go. After we found the general direction, we tried out four different approaches to retrieve nuggets and picked one of them. After it was fine tuned, we generated our nuggets as input to the next pipeline step.
\subsubsection{Data Analysis}
\textbf{Frequency of Nuggets} \\
As a first step, we wanted to get an idea how rare nuggets were in the manually annotated corpus that was given to us. Therefore we calculated the ratio of nuggets to all the source sentences over the different topics. We found out that, depending on the topic, between 9.3\% to 24.9\% of sentences were nuggets or contained a nugget. The mean ratio over all topics was at 14.1\%. Knowing the distribution of positive and negative examples which in our case are nuggets and no nuggets, is important, because it directly affects the learning process of the later classifier. If for example there are only 10\% positive examples, a classifier may choose to always predict the majority class, which would be the negative one, no nugget. This leads to a high accuracy but is not what we want. When we knew these numbers, we can multiply the amount of positive examples to reach something close to a 50:50 distribution. Hereby we would increase the cost of classifying a positive instance wrong and therefore have more control over the learning process of the classifier. \newline

\textbf{Structure of nuggets}\\
After that, we looked at the nuggets in more detail. Particularly, we wanted to know how many nuggets were in fact whole sentences. "Whole sentence", in this context, means that there is a 1:1 counterpart in the relevant sentences input file. Unfortunately, it was not that easy since the nuggets seemed to have another preprocessing than the source sentences, especially they were in tokenized form which is why we could not simply compare these 1:1. Moreover, we did not want to have such a strict 1:1 relation since we noticed that a lot of nuggets were somehow really close to the source sentence but not identical. The example below clarifies what we mean by that. \\

\textit{Source sentence:} \\ 
\textit{However, we have found that stimulants, and other prescription drugs, are not the only effective interventions for ADHD.} \\

\textit{Selected nugget:} \\
\textit{we have found that stimulants, and other prescription drugs, are not the only effective interventions for ADHD.} \\

The above sentences actually only differ by the starting word "However" which can easily left out. We wanted to threat these kinds of nuggets also as "whole sentences". We solved this by using difflib \lbrack \url{https://docs.python.org/2/library/difflib.html}\rbrack, which gives a score of how similar two sequences of characters are. We chose a threshold of 95\% and declared every pair of sentences with a similarity above that threshold to be the same. As a result, we found that depending on the topic between 70.5\% and 87.9\% of nuggets were whole sentences with a mean of 82\%. This ratio was surprisingly high and led us to a big simplification we have made, which is to only focus on full sentences for our content selection. This would also be useful for our actual summarization task at the end of the pipeline. Therefore, the core problem we had to tackle was to decide whether a given sentence is a nugget or not.\\
We further analyzed the structure of nuggets by searching for patterns like certain words that appear primarily in nuggets or no nuggets. We thought that words like "significant" or "important" could indicate that there is relevant information content in this sentence and thus would make good features. Unfortunately, it turned out that words are very much equally distributed over both types of sentences. Which words appear most often is, as you would expect, topic dependend.


\subsubsection{Training Classifiers}
After gaining sufficient insights into the data we were given, we created binary labeled versions of them. For preprocessing we used gensims simple preprocess \lbrack\url{https://radimrehurek.com/gensim/utils.html}\rbrack . We thought about different approaches to train a classifier and ended up with four general variants. Most of our approaches did use word embeddings for transforming the source sentence into a reasonable input for the classifier. In the beginning we did try out to train embeddings ourselves with the complete corpus that was given to us and the FastText as well as the Word2Vec framework. This had the benefit of not having any out-of-vocabulary words for our task but it is very likely that the corpus was not sufficiently big enough to train really good embeddings. That is why we later have choosen to use pre-trained embeddings instead. Namely, in our neural network approaches we used the $300$-dimensional GloVe embeddings from Stanford (\url{https://nlp.stanford.edu/projects/glove/}) which have been trained on 6 billion tokens from Wikipedia and Gigaword. \\

\textbf{1. Approach: FastText}\\
FastText \cite{joulin2016bag} is an open-source library for text classification and word embeddings developed by Facebook AI Research (FAIR) lab. Since it already provides an implemenation for text classification using word embeddings, we just had to transform our data into the right format and potentially change the ratio of positive and negative examples in order to control the learning process of the classifier. \lbrack nachdem ich hier nochmal rumprobiert habe, bekomme ich Recall = 0.1617, Precision = 0.8175, was besser erscheint als es sein sollte! Was fangen wir jetzt damit an? \rbrack. Because of ... we did not trust these results?... \\

\textbf{2. Approach: Naive Bayes}\\
We also wanted to try out a simple approach which would not use any word embeddings but only processes the raw tokens somehow. Therefore we tried out the Naive Bayes classifiers and experimented with some different features. For example, we used the sentence length with discretized intervals as a feature as well as the position of the sentence in its document. Furthermore, we used a POS tagger to create a binary feature of whether a verb is present or not. Unfortunately the results were very bad. That is because first of all, we did not find enough good features and second, the POS tagger could not reliably tell whether a sentence contained a verb or not.\\
This approach became interesting when we included 10 further features of which each one corresponded to one of the top 10 most frequent words in that topic and whether it is present in the sentence or not. This resulted in a recall of 37.8\% and a precision of 19.5\%, which is actually something you would be able to work with. The downside to this is, that you can train a classifier only for one specific topic and you need training data for it. So there is no way to retrieve nuggets for a topic without training data. In conclusion we can say, that this approach did not work quite well for our purposes.\\

\textbf{3. Approach: Fully Connected Neural Network}\\
As already mentioned, we used pre-trained embeddings for this approach. For the neural network architecture we had the challenge to transform every sentence into a vector of fixed length. A natural way to do so, is to filter out stopwords of the sentence and average the word embeddings of the remaining words and it often delivers quite reasonable results as indicated by \cite{iyyer2015deep}. So this is what we have done as a first step, although we knew that this indeed looses some information already such as the word order in the sentence. In later variants, we also included the context sentences of the actual sentence into the input of the network. To be more precise, we included the $k$ preceding and succeeding sentences (according to the document in the xml source file) where $k$ was our window size.\\
We implemented our network with Keras and in the end we did a hyperparameter optimization over the import parameters like the above mentioned window size, the number of hidden layers in the network, the dimensions of the layers, the activation functions and the optimizer. As a result we obtained a precision of 35\% and a recall of 1.5\%. These results seemed not that strong to us and we thought that the very simple idea of averaging all the embeddings of the input tokens would be a major problem.\\

\textbf{4. Approach: Convolutional Neural Network}\\
In order to omit the problem of averaging the embeddings of the input tokens, we tried out a convolutional neural network which was in the end our best system and therefore the one we used for the nugget selection. The major advantage of this architecture is that it learns filters that are processed over the embeddings of the input tokens and therefore can also take the word order into account and can learn which sequences of tokens are important and which are not. In contrast to the neural network architecture before, we did not filter stopwords here since we thought they could be relevant for the structure of the sentence here. So we only did the simple preprocessing which tokenized and lowercased the sentence. Moreover, we did not include the context sentences here but classified each sentence in isolation. \\
We did make a restriction for the input sentences in this approach, which was exclude sentences from our classifying process which had fewer than 5 oder more than 50 tokens. We just thought of those sentences of not beeing very useful in a summarization at the end, especially because our restriction was to have 600 characters in total in the end. We also filtered out sentences with containing phrases or characters like "I", "?", "We" in order to filter out personal opinions.\\
We also did a hyperparameter search on a random base here as well and also used early stopping in the actual training process. After testing some diverse parameter configurations and got an idea of what generally works, we defined ranges for each of them. The program received these ranges, as well as the training, deveopment and validation set. It generated random configurations within the given ranges, trained the network on the training set and evaluated the model after each epoch. If performance decreased on the development set, the program finished training in order to prevent overfitting and to save ressources. If the new model beat the currently best one, it is stored and the best results are udated. Since this task is very computation heavy, we used the Lichtenberg Cluster \lbrack Link...\rbrack to do this for us. As a general restraint for the results, we decided to only allow models that have a recall of at least 5\% but a precision as high as possible. Recall and precision seem to behave anti-proportional, therefore we have to choose which one them is more important to us. Since we want to create meaningful and well readable summarizations in the end which are not very long anyway, precision is of a much greater importance to us. \\
Our final parameters provided a recall of 5.9\% and a precision of 52\%. We noticed that for some topics that had a high number of documents the amount of predicted nuggets is very large as well and for other smaller topics the number of nuggets was very small. This led us to the decision that we always use the 30 sentences with the highest probability of being nuggets as input to the next pipeline step. This guarantees enough material to work with in the later steps while using only the nuggets we are most confident in if we would have predicted more.\\
